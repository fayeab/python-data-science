{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Le modèle BERT (Bidirectional Encoder Representations from Transformers) a été concu par Google AI en 2018. Il s'agit d'un modèle non supervisé. Son objecif est de fournir une représention vectorielle du langage. BERT est une architecture de réseaux de neurones basé sur les [transformers](python-nlp-transformers.ipynb) bidirectionnels. Ces derniers utilisent un [mécanisme d'attention](python-nlp-mecanisme-attention.ipynb) pour construire des embeddings contextualisés.\n",
    "<img src=\"images/bert/bert_model.png\" >\n",
    "\n",
    "Dans le papier original, ils ont présenté deux modèles :\n",
    "* Base : modèle de base de BERT\n",
    " * 12 couches\n",
    " * 768 dimensions cachées  \n",
    " * 12 têtes d’attention\n",
    " * 110M paramètres\n",
    "* Large : \n",
    " * 24 couches \n",
    " * 1024 dimensions cachées\n",
    " * 16 têtes d’attention\n",
    " * 340M paramètres\n",
    "\n",
    "<img src=\"images/bert/bert_layers.png\" >\n",
    "\n",
    "## 2. Apprentissage de BERT\n",
    "\n",
    "BERT a été entrainé de façon non supervisée sur deux tâches :\n",
    "\n",
    "* **Masked Language Model (MLM)** : BERT utilise le MLM pour apprendre une représentation bidirectionnelle d'une séquence. En effet le MLM apprend à prédire des mots masqués dans une phrase. Le modèle masque aléatoirement 15% des tokens de la séquence en entrée, puis utilise les tokens qui restent pour prédire les tokens masqués.\n",
    "\n",
    "* **Next Sentence Prediction (NSP)** : BERT se base sur le NSP pour apprendre la relation entres les séquences. Ce qui lui permet de savoir si deux séquences ont un lien logique et séquentiel ou si leur relation est simplement aléatoire.\n",
    "\n",
    "\n",
    "## 3. Comment utiliser BERT\n",
    "\n",
    "Les modèles pré-entrainés de BERT peuvent être exploités avec les techniques du Transfert Learning :\n",
    "\n",
    "* Fine Tuning : ajouter une couche prédictive au dessus du modèle BERT. Le modèle sera ré-entraîné plus finement pour effectuer une nouvelle tâche d'apprentissage supervisé.\n",
    "* Feature Based : extraire des caractéristiques. Le modèle fournit une représentation vectorielle des inputs.\n",
    "\n",
    "BERT est peut-être utilisé pour répondre à plusieurs problématiques de NLP :\n",
    "\n",
    "* Génération de texte\n",
    "* Classification\n",
    "* Question-réponse\n",
    "* Reconnaissance d’entités nommées\n",
    "* Traduction\n",
    "\n",
    "\n",
    "# 4. Quelques adaptations de BERT\n",
    "\n",
    "Plusieurs modèles ont été construits autour BERT :\n",
    "\n",
    "* roBERTa : version optimisée de BERT proposé par Facebook\n",
    "* FlauBERT :  un modèle BERT pré-entraîné sur un vocabulaire en français développé par les universités de Grenoble, de Paris Diderot et le CNRS.\n",
    "* CamemBERT : un modèle roBERTa pré-entraîné sur un vocabulaire en français proposé par l'Inria et Facebook\n",
    "\n",
    "**Références :**\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)   \n",
    "[Blog Post by Google AI](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)     \n",
    "[Colab Notebook: Predicting Movie Review Sentiment with BERT on TF Hub](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=xiYrZKaHwV81)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Application : Analyse de sentiments\n",
    "\n",
    "Ce travail s'inpire du post d'Aniruddha Choudhury sur [Medium](https://medium.com/@aniruddha.choudhury94/part-2-bert-fine-tuning-tutorial-with-pytorch-for-text-classification-on-the-corpus-of-linguistic-18057ce330e1) et du [dépôt Git](https://github.com/google-research/bert) de Google Research\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "    data = {}\n",
    "    data[\"sentence\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    for file_path in os.listdir(directory):\n",
    "        with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "            data[\"sentence\"].append(f.read())\n",
    "            data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "    pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "    neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "    pos_df[\"polarity\"] = 1\n",
    "    neg_df[\"polarity\"] = 0\n",
    "    return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "\n",
    "    train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "    test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(10000)\n",
    "test = test.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation du tokenizer de BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Préprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def encode_sentences(sentences, max_length=512):\n",
    "    \"\"\"\n",
    "     `encode` will:\n",
    "        (1) Tokenize the sentence.\n",
    "        (2) Prepend the `[CLS]` token to the start.\n",
    "        (3) Append the `[SEP]` token to the end.\n",
    "        (4) Map tokens to their IDs.\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_length,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "        input_ids.append(encoded_sent)\n",
    "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", \n",
    "                              value=0, truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "\n",
    "def prepare_data(inputs, labels, masks=None, training=True, batch_size=16):\n",
    "    # Convert all inputs and labels into torch tensors, the required datatype for our model.\n",
    "\n",
    "    inputs = torch.tensor(inputs)\n",
    "    labels = torch.tensor(labels)\n",
    "    masks = torch.tensor(masks)\n",
    "    data = TensorDataset(inputs,\n",
    "                         masks, \n",
    "                         labels)\n",
    "    if training:\n",
    "        # Create the DataLoader for training set.\n",
    "        sampler = RandomSampler(data)\n",
    "    else:\n",
    "        # Create the DataLoader for validation set.\n",
    "        sampler = SequentialSampler(data)\n",
    "\n",
    "        \n",
    "    dataloader = DataLoader(data,\n",
    "                            sampler=sampler,\n",
    "                            batch_size=batch_size)\n",
    "    return dataloader\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train.sentence.values\n",
    "labels = train.polarity.values\n",
    "random_state = 12345\n",
    "batch_size = 16\n",
    "max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for\n",
    "# training\n",
    "# Use 90% for training and 10% for validation.\n",
    "\n",
    "input_ids, attention_masks = encode_sentences(sentences, max_length=max_length)\n",
    "\n",
    "(train_inputs, validation_inputs,\n",
    "train_labels, validation_labels) = train_test_split(input_ids,\n",
    "                                                    labels, \n",
    "                                                    random_state=random_state,\n",
    "                                                    test_size=0.1)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks,\n",
    "                                                       labels,\n",
    "                                                       random_state=random_state,\n",
    "                                                       test_size=0.1)\n",
    "\n",
    "train_dataloader = prepare_data(inputs=train_inputs, \n",
    "                                labels=train_labels, \n",
    "                                masks=train_masks, \n",
    "                                training=True,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "validation_dataloader = prepare_data(inputs=train_inputs, \n",
    "                                     labels=train_labels, \n",
    "                                     masks=train_masks, \n",
    "                                     training=False,\n",
    "                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importation du modèle BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "optimizer = AdamW(model.parameters(), lr = 5e-5,  eps = 1e-7)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import random\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "seed_val = 12345\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        b_input_ids = batch[0].to(device).long()\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        model.zero_grad()        \n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    # Tracking variables \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        b_input_ids = b_input_ids.long()\n",
    "        with torch.no_grad():        \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "    # Report the final accuracy for this validation run.\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "f = pd.DataFrame(loss_values)\n",
    "f.columns=['Loss']\n",
    "fig = px.line(f, x=f.index, y=f.Loss)\n",
    "fig.update_layout(title='Training loss of the Model',\n",
    "                   xaxis_title='Epoch',\n",
    "                   yaxis_title='Loss')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test.sentence.values\n",
    "labels = test.polarity.values\n",
    "\n",
    "input_ids, attention_masks = encode_sentences(sentences, max_length=max_length)\n",
    "\n",
    "prediction_dataloader = prepare_data(inputs=input_ids, \n",
    "                                     labels=labels, \n",
    "                                     masks=attention_masks, \n",
    "                                     training=False,\n",
    "                                     batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Prediction on test set\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(sentences)))\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids = b_input_ids.long()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import (confusion_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "sns.set()\n",
    "rcParams['figure.figsize'] = 7, 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(flat_true_labels, flat_predictions)\n",
    "print('Accuracy : %.3f' % acc)\n",
    "\n",
    "\n",
    "f1_s = f1_score(flat_true_labels, flat_predictions)\n",
    "print('F1-Score : %.3f' % f1_s)\n",
    "\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"0\", \"1\"]\n",
    "conf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n",
    "conf_matrix = 100 * conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(conf_matrix, xticklabels=class_names, yticklabels=class_names, annot=True, fmt=\".2f\", cbar=False);\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.ylabel('Valeurs réalisées')\n",
    "plt.xlabel('Valeurs prédites')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
