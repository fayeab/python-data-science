{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sélection de variables\n",
    "\n",
    "La sélection de variables consiste à chercher un sous-ensemble de variables parmi un ensemble de variables. Les variables sélectionnées ne se sont pas tranformés et gardent ainsi leur signification initiale. La sélection des variables permet de trouver un modèle parcimonieux. Cependant il souvent difficile de trouver un sous-ensemble de variables stable même avec une validation croisée.\n",
    "\n",
    "**Critères de sélection**\n",
    "\n",
    "Il existe plusieurs critères permettant de mesurer la qualité d'une prédiction :\n",
    "* Le critère d’information d'Akaïke (AIC) : retient les variables pertinentes lors de prévisions\n",
    "* Le critère d’information bayésien (BIC) : vise la sélection de variables statistiquement significatives\n",
    "dans le modèle.   \n",
    "* Les métriques d'évaluation de modèles (MSE, MAE, accuracy, ...)\n",
    "* ...\n",
    "\n",
    "Ces critères sont équivalents aux R2, lorsqu'on compare des modèles avec le même nombre de variables.\n",
    "\n",
    "**Remarque** :   \n",
    "Dans le cadre d'une sélection de variables, le R2 ne permet pas d'obtenir le modèle optimale car il préviligie le modèle complexe (celui avec plus de variables).\n",
    "\n",
    "L'objectif est de montrer comment mettre en oeuvre les techniques de sélection de variables avec Python. Il ne s'agira pas comparer les résultats des différentes méthodes de sélection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O3OBS          float64\n",
       "MOCAGE         float64\n",
       "TEMPE          float64\n",
       "VENTMOD        float64\n",
       "VENTANG        float64\n",
       "SRMH2O         float64\n",
       "LNO2           float64\n",
       "LNO            float64\n",
       "JOUR_1           uint8\n",
       "STATION_Als      uint8\n",
       "STATION_Cad      uint8\n",
       "STATION_Pla      uint8\n",
       "STATION_Ram      uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt, log\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = pd.read_csv('data/depSeuil.txt', sep=\",\")\n",
    "data.columns = data.columns.str.upper()\n",
    "data[\"STATION\"] = pd.Categorical(data[\"STATION\"], ordered=False)\n",
    "data[\"JOUR\"] = pd.Categorical(data[\"JOUR\"], ordered=False)\n",
    "data[\"O3OBS\"] = pd.DataFrame(data[\"O3OBS\"], dtype=np.float64)\n",
    "\n",
    "data[\"SRMH2O\"] = data[\"RMH2O\"].map(lambda x: sqrt(x))\n",
    "data[\"LNO2\"] = data[\"NO2\"].map(lambda x: log(x))\n",
    "data[\"LNO\"] = data[\"NO\"].map(lambda x: log(x))\n",
    "\n",
    "data = data.drop([\"RMH2O\", \"NO2\", \"NO\"], axis=1)\n",
    "numerical_features = [\"MOCAGE\", \"TEMPE\", \"VENTMOD\", \"VENTANG\", \"SRMH2O\", \"LNO2\", \"LNO\"]\n",
    "categorical_features = [\"JOUR\", \"STATION\"]\n",
    "\n",
    "data = pd.get_dummies(data, columns=categorical_features, drop_first=True)\n",
    "\n",
    "target_name = 'O3OBS'\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=125)\n",
    "X_train = train.drop(target_name, axis=1)\n",
    "Y_train = train[target_name]\n",
    "X_test = test.drop(target_name, axis=1)\n",
    "Y_test = test[target_name]\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sélection de variable par pénalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une régularisation **Lasso** type `L1` peut-être utilisée pour sélectionner des variables. On peut utiliser la version randomisée pour éviter l’instabilité des résultats. Les variables sélectionnées ont un coefficient non nul.\n",
    "\n",
    "`Scikit-Learn` fournit un estimateur permet de récupérer les variables sélectionnées. Elles pourront être utilisées par un autre méthode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de variables intiales : 12\n",
      "Nombre de variables sélectionnées par le Lasso : 6\n",
      "MSE = 591.12\n",
      "Wall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "print('Nombre de variables intiales : {}'.format(X_train.shape[1]))\n",
    "\n",
    "pipe = Pipeline([('feature_selection', SelectFromModel(Lasso(random_state=125))),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "print('Nombre de variables sélectionnées par le Lasso : {}'.format(model.best_estimator_[-1].n_features_))\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression **Ridge** conserve toutes les variables mais empêche les coefficients de prendre de trop grandes valeurs et limite ainsi la variance des prévision\n",
    "\n",
    "La méthode **Elastic Net** permet de combiner la régression **ridge** et la régression **Lasso**, en introduisant les deux types de pénalités simultanément."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recherche du meilleur modèle par sélection de variables\n",
    "\n",
    "## 2.1 Recherche exhaustive\n",
    "\n",
    "La recherche exhaustive consiste à tester toutes les combinaisons de variables pour sélectionner le meilleur modèle.\n",
    "\n",
    "Cette solution est à éviter lorsqu'on a beaucoup de données car il y a 2<sup>p</sup> - 1 modèles à évaluer (`p` = nombre de variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 La recherche pas à pas\n",
    "\n",
    "## 2.2.1 Méthode descendante : back elimination\n",
    "\n",
    "* On part du modèle complet avec toutes variables explicatives\n",
    "* A chaque étape, on envèle la variable conduit au critère de sélection le plus faible\n",
    "* La procédure s’arrête lorsque le critère ne décroît plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 592.14\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(random_state=125)\n",
    "\n",
    "pipe = Pipeline([('feature_selection', SequentialFeatureSelector(lasso, \n",
    "                                                                 n_features_to_select=6, \n",
    "                                                                 direction='backward')),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2  Méthode ascendante : foward selection\n",
    "\n",
    "* À chaque étape, une variable est ajoutée au modèle. Il s'agit la variable qui permet de réduire au mieux le critère de sélection \n",
    "\n",
    "* La procédure s’arrête lorsque toutes les variables sont introduites ou lorsque le critère de sélection ne décroît plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 592.14\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(random_state=125)\n",
    "\n",
    "pipe = Pipeline([('feature_selection', SequentialFeatureSelector(lasso, \n",
    "                                                                 n_features_to_select=6, \n",
    "                                                                 direction='forward')),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.3  Stepwise selection\n",
    "\n",
    "C'est un mixte de forward selection et backward elimination. Il introduit une étape d'élimination de variable après chaque étape de sélection afin de retirer du modèle d'éventuels variables qui seraient devenues moins indispensables du fait de la présence de celles nouvellement introduite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. D'autres techniques de sélection de variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Recursive feature elimination (RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode RFE fonctionne par suppression récursive des features en se basant sur l'importance des variables :\n",
    "* Le nombre de variables à sélectionner est fixé au départ.\n",
    "* On part sur le modèle complet (avec toutes les variables explicatives)\n",
    "* A chaque étape, on  élimine la variable la moins importante\n",
    "* Jusqu'à atteindre le nombre de variables à sélectionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 696.60\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline([('feature_selection', RFE(SVR(kernel=\"linear\"), n_features_to_select=6, step=1)),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Recursive feature elimination with cross-validation (RFECV)\n",
    "\n",
    "Comme son nom l'indique, le RFECV ajoute une cross-validation au RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 588.90\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline([('feature_selection', RFECV(SVR(kernel=\"linear\"),\n",
    "                                             min_features_to_select=6,\n",
    "                                             step=1,\n",
    "                                             cv=3)),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Suppression des variables à faible variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-Learn` fournit un estimateur permet de filter les variables avec une faible variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 586.23\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pipe = Pipeline([('feature_selection', VarianceThreshold(threshold=0.1)),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Sélection des variables basée sur un test statistique\n",
    "\n",
    "`SelectKBest` permet de sélectionner les `k` meilleurs prédicteurs à l'aide d'une fonction score :\n",
    "* Classification : chi2, f_classif, mutual_info_classif\n",
    "* Regression: f_regression, mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 658.63\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "pipe = Pipeline([('feature_selection',  SelectKBest(f_regression, k=6)),\n",
    "                 ('regressor', RandomForestRegressor(random_state=125))])\n",
    "\n",
    "param_grid = {\n",
    "              \"regressor__max_features\": [0.6, 0.8],\n",
    "              \"regressor__n_estimators\": [50, 100, 200, 300],\n",
    "              \"regressor__min_samples_split\": [2, 3, 4],\n",
    "              \"regressor__min_samples_leaf\": [2, 3, 4]\n",
    "             }\n",
    "\n",
    "model = GridSearchCV(pipe, param_grid, cv=3, n_jobs=3)\n",
    "_ = model.fit(X_train, Y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(Y_test, predictions)\n",
    "print(\"MSE = {0:.2f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**    \n",
    "[Sélection de modèle en régression linéaire, Philippe Besse](https://www.math.univ-toulouse.fr/~besse/Wikistat/pdf/st-m-app-linSelect.pdf)  \n",
    "[Scikit-learn Documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
