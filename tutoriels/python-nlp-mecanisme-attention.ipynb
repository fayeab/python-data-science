{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modèle séquentiel est un modèle qui prend une entrée une séquence d'items et prédit une séquence. Ce type de modèle permet d'effectuer des tâches comme :\n",
    "\n",
    "* Traduction automatique qui traduit une séquence d'une langue donnée dans une autre\n",
    "* Sous-titrage d'images\n",
    "* Résumé de textes\n",
    "* ..\n",
    "\n",
    "La plupart des modèles séquentiels sont basés sur une utilisation des réseaux de neurones récurrents. Ce type réseau a des inconvénients entre autres :\n",
    "* Le temps de calcul est long \n",
    "* Les phénomènes de gradient qui disparait et qui explose malgré l'introduction des LSTM ((Long Short-Term Memory Network)\n",
    "* Ils sont difficilement parallélisable du fait de leur nature\n",
    "* ..\n",
    "\n",
    "Pour résoudre ces problèmes, Google a proposé un modèle d'architecture appelé `Transformer` permettant d'éviter la récurrence en utilisant un mécanisme d'`attention`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le mécanisme d'attention\n",
    "\n",
    "Initialement le mécanisme d'attention a été conçu pour prendre en compte la position des tokens au moment d'une séquence. Il permet de mettre en valeur les parties plus importantes de la séquence. Au moment générer une nouvelle partie de la séquence de sortie, l'attention permet au décodeur de prendre en compte le contexte à partir des vecteurs d'états issus de l'encodage.\n",
    "\n",
    "Mathématiquement l'attention est une fonction qui prend en entrée une requête et un ensemble de paires clé-valeur fournit en sortie un score. Le score est une somme pondérée des valeurs. La pondération de chaque valeur dépend de la relation entre la requête et la clé correspondantes. Les scores sont normalisés par la fonction `softmax`.\n",
    "\n",
    "En pratique, cela revient à un calcul matriciel puisqu'on traite simultanément plusieurs requêtes et que chaque requête est représenté par un vecteur (de même que les clés et leurs valeurs).\n",
    "\n",
    "<img src=\"images/attention/attention.png\">\n",
    "\n",
    "[Ce code permet d'illuster ce calcul.](http://nlp.seas.harvard.edu/2018/04/03/attention.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    \n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Head Attention\n",
    "\n",
    "On peut associer plusieurs fonctions d'attention. \n",
    "\n",
    "<img src=\"images/attention/multi-head-attention-graph.png\">\n",
    "\n",
    "Cela permet de représenter les vecteurs requêtes, clés et valeurs dans plusieurs sous-espaces.\n",
    "\n",
    "Les différents projections sont concaténés pour obtenir le résultat final.\n",
    "\n",
    "<img src=\"images/attention/multi-head-attention-formule.png\">\n",
    "\n",
    "[Ce code permet d'illuster ce calcul.](http://nlp.seas.harvard.edu/2018/04/03/attention.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.linear_query = nn.Linear(d_model, d_model)\n",
    "        self.linear_value = nn.Linear(d_model, d_model)\n",
    "        self.linear_key = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        selt.attn = None\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bs = query.size(0)\n",
    "        key = self.linear_key(key).view(bs, -1, self.h, self.d_k)\n",
    "        query = self.linear_key(query).view(bs, -1, self.h, self.d_k)\n",
    "        value = self.linear_view(values).view(bs, -1, self.h, self.d_k)\n",
    "       \n",
    "        key = key.transpose(1, 2)\n",
    "        query = query.transpose(1, 2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        scores, self.attn = attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "   \n",
    "        return self.output(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention (auto-attention)\n",
    "\n",
    "L'auto-attention est très similaire à l'attention mais elle permet intégrer l'interaction entre différentes parties de la séquence.   \n",
    "Ainsi elle permet de détecter les relations entre les tokens.\n",
    "\n",
    "Cet exemple présenté par [Google AI](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) permet d'illustrer l'application du mécanisme d'auto-attention à la résolution des coréférences.\n",
    "\n",
    "<img src=\"images/attention/self-attention.png\">\n",
    "\n",
    "Le mécanisme d'attention permet de faire la différence entre les deux mots `it`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Références:**  \n",
    "[Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)  \n",
    "[Harvard NLP, The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  \n",
    "[Transformer: A Novel Neural Network Architecture for Language Understanding](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
